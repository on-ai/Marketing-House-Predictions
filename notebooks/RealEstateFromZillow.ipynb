{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requests \n",
    "# !pip install bs4\n",
    "# !pip install pyppeteer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow_url = \"https://www.zillow.com/homes/Redding,-CA_rb/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_html_info(url):\n",
    "    # Fetch the HTML content\n",
    "    response = requests.get(url)\n",
    "    html_content = response.content\n",
    "\n",
    "    # Parse the HTML using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Dump all the classes\n",
    "    print(\"Classes:\")\n",
    "    for class_name in soup.find_all(class_=True):\n",
    "        print(class_name.get('class'))\n",
    "\n",
    "    # Dump all the nodes\n",
    "    print(\"\\nNodes:\")\n",
    "    for node in soup.find_all():\n",
    "        print(node.name)\n",
    "\n",
    "    # Dump all the properties\n",
    "    print(\"\\nProperties:\")\n",
    "    for node in soup.find_all():\n",
    "        for key, value in node.attrs.items():\n",
    "            print(f\"{node.name}.{key} = {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyppeteer import launch\n",
    "\n",
    "async def dump_html_tree(url):\n",
    "    # Launch a new browser instance\n",
    "    browser = await launch()\n",
    "    page = await browser.newPage()\n",
    "\n",
    "    # Navigate to the URL\n",
    "    await page.goto(url)\n",
    "\n",
    "    # Get the HTML content\n",
    "    html_content = await page.content()\n",
    "\n",
    "    # Parse the HTML using BeautifulSoup\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    def recursive_dump(node, indent=0):\n",
    "        # Print the current node's tag name and attributes\n",
    "        print(' ' * indent + node.name)\n",
    "        for key, value in node.attrs.items():\n",
    "            print(' ' * (indent + 2) + f\"{key} = {value}\")\n",
    "\n",
    "        # Recursively dump the child nodes\n",
    "        for child in node.children:\n",
    "            if isinstance(child, bs4.element.Tag):\n",
    "                recursive_dump(child, indent + 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another paid service $0.5 for 1,000 captchas solved, with a down payment of $10\n",
    "from time import sleep\n",
    "\n",
    "# 2Captcha API key\n",
    "API_KEY = 'YOUR_2CAPTCHA_API_KEY'\n",
    "\n",
    "def solve_captcha(image_url):\n",
    "    # Download the captcha image\n",
    "    response = requests.get(image_url)\n",
    "    with open('captcha.png', 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    # Upload the captcha image to 2Captcha\n",
    "    payload = {'key': API_KEY, 'method': 'post', 'url': image_url}\n",
    "    response = requests.post('http://2captcha.com/in.php', data=payload)\n",
    "    captcha_id = response.text.split('|')[1]\n",
    "\n",
    "    # Wait for the captcha to be solved\n",
    "    while True:\n",
    "        payload = {'key': API_KEY, 'action': 'get', 'id': captcha_id}\n",
    "        response = requests.get('http://2captcha.com/res.php', params=payload)\n",
    "        if response.text.startswith('OK'):\n",
    "            captcha_solution = response.text.split('|')[1]\n",
    "            return captcha_solution\n",
    "        sleep(5)\n",
    "\n",
    "# Example usage\n",
    "captcha_image_url = 'https://example.com/captcha.png'\n",
    "captcha_solution = solve_captcha(captcha_image_url)\n",
    "print(f'Captcha solution: {captcha_solution}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes:\n",
      "\n",
      "Nodes:\n",
      "html\n",
      "head\n",
      "meta\n",
      "meta\n",
      "meta\n",
      "title\n",
      "body\n",
      "script\n",
      "\n",
      "Properties:\n",
      "html.lang = en\n",
      "meta.charset = utf-8\n",
      "meta.name = viewport\n",
      "meta.content = width=device-width, initial-scale=1\n",
      "meta.name = description\n",
      "meta.content = px-captcha\n"
     ]
    }
   ],
   "source": [
    "dump_html_info(zillow_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object dump_html_tree at 0x00000245DA759740>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump_html_tree(zillow_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(zillow_url, headers = header )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4672\n"
     ]
    }
   ],
   "source": [
    "print(len(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE html>\\n<html lang=\"en\">\\n<head>\\n    <meta charset=\"utf-8\">\\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\n    <meta name=\"description\" content=\"px-captcha\">\\n    <title>Access to this page has been denied</title>\\n    \\n</head>\\n<body>\\n<script>\\n    /* PerimeterX assignments */\\n    window._pxVid = \\'\\';\\n    window._pxUuid = \\'54ce22c9-0af4-11ef-a28e-bb5b1e974e30\\';\\n    window._pxAppId = \\'PXHYx10rg3\\';\\n    window._pxMobile = false;\\n    window._pxHostUrl = \\'/HYx10rg3/xhr\\';\\n    window._pxCustomLogo = \\'https://www.zillowstatic.com/s3/pfs/static/z-logo-default.svg\\';\\n    window._pxJsClientSrc = \\'/HYx10rg3/init.js\\';\\n    window._pxFirstPartyEnabled = true;\\n    var pxCaptchaSrc = \\'/HYx10rg3/captcha/captcha.js?a=c&u=54ce22c9-0af4-11ef-a28e-bb5b1e974e30&v=&m=0\\';\\n\\n    var script = document.createElement(\\'script\\');\\n    script.src = pxCaptchaSrc;\\n    script.onerror = function () {\\n        script = document.createElement(\\'script\\');\\n        script.src = \\'https://captcha.px-cloud.net/PXHYx10rg3/captcha.js?a=c&amp;u=54ce22c9-0af4-11ef-a28e-bb5b1e974e30&amp;v=&amp;m=0\\';\\n        script.onerror = window._pxOnError;\\n        document.head.appendChild(script);\\n    };\\n    window._pxOnError = function () {\\n        var style = document.createElement(\\'style\\');\\n        style.innerText = \\'@import url(https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap);body{background-color:#fafbfc}.px-captcha-error-container{position:fixed;height:340px;background-color:#fff;font-family:Roboto,sans-serif}.px-captcha-error-header{color:#f0f1f2;font-size:29px;margin:67px 0 33px;font-weight:500;line-height:.83;text-align:center}.px-captcha-error-message{color:#f0f1f2;font-size:18px;margin:0 0 29px;line-height:1.33;text-align:center}.px-captcha-error-button{text-align:center;line-height:48px;width:253px;margin:auto;border-radius:50px;border:solid 1px #f0f1f2;font-size:20px;color:#f0f1f2}.px-captcha-error-wrapper{margin:18px 0 0}div.px-captcha-error{margin:auto;text-align:center;width:400px;height:30px;font-size:12px;background-color:#fcf0f2;color:#ce0e2d}img.px-captcha-error{margin:6px 8px -2px 0}.px-captcha-error-refid{border-top:solid 1px #f0eeee;height:27px;margin:13px 0 0;border-radius:0 0 3px 3px;background-color:#fafbfc;font-size:10px;line-height:2.5;text-align:center;color:#b1b5b8}@media (min-width:620px){.px-captcha-error-container{width:530px;top:50%;left:50%;margin-top:-170px;margin-left:-265px;border-radius:3px;box-shadow:0 2px 9px -1px rgba(0,0,0,.13)}}@media (min-width:481px) and (max-width:620px){.px-captcha-error-container{width:85%;top:50%;left:50%;margin-top:-170px;margin-left:-42.5%;border-radius:3px;box-shadow:0 2px 9px -1px rgba(0,0,0,.13)}}@media (max-width:480px){body{background-color:#fff}.px-captcha-error-header{color:#f0f1f2;font-size:29px;margin:55px 0 33px}.px-captcha-error-container{width:530px;top:50%;left:50%;margin-top:-170px;margin-left:-265px}.px-captcha-error-refid{position:fixed;width:100%;left:0;bottom:0;border-radius:0;font-size:14px;line-height:2}}@media (max-width:390px){div.px-captcha-error{font-size:10px}.px-captcha-error-refid{font-size:11px;line-height:2.5}}\\';\\n        document.head.appendChild(style);\\n        var div = document.createElement(\\'div\\');\\n        div.className = \\'px-captcha-error-container\\';\\n        div.innerHTML = \\'<div class=\"px-captcha-error-header\">Before we continue...</div><div class=\"px-captcha-error-message\">Press & Hold to confirm you are<br>a human (and not a bot).</div><div class=\"px-captcha-error-button\">Press & Hold</div><div class=\"px-captcha-error-wrapper\"><div class=\"px-captcha-error\"><img class=\"px-captcha-error\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABMAAAAQCAMAAADDGrRQAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAABFUExURUdwTNYELOEGONQILd0AONwALtwEL+AAL9MFLfkJSNQGLdMJLdQJLdQGLdQKLtYFLNcELdUGLdcBL9gFL88OLdUFLNEOLglBhT4AAAAXdFJOUwC8CqgNIRgRoAS1dWWuR4RTjzgryZpYblfkcAAAAI9JREFUGNNdj+sWhCAIhAdvqGVa1r7/oy6RZ7eaH3D4ZACBIed9wlOOMtUnSrEmZ6cHa9YAIfsbCkWrdpi/c50Bk2CO9mNLdMAu03wJA3HpEnfpxbyOg6ruyx8JJi6KNstnslp1dbPd9GnqmuYq7mmcv1zjnbQw8cV0xzkqo+fX1zkjUOO7wnrInUTxJiruC3vtBNRoQQn2AAAAAElFTkSuQmCC\">Please check your internet connection\\' + (window._pxMobile ? \\'\\' : \\' or disable your ad-blocker\\') + \\'.</div></div><div class=\"px-captcha-error-refid\">Reference ID \\' + window._pxUuid + \\'</div>\\';\\n        document.body.appendChild(div);\\n        if (window._pxMobile) {\\n            setTimeout(function() {\\n                location.href = \\'/px/captcha_close?status=-1\\';\\n            }, 5000);\\n        }\\n    };\\n    document.head.appendChild(script);\\n</script>\\n\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = soup.find('address', attrs={'data-test': 'property-card-addr'})\n",
    "\n",
    "# 'address' will contain the address you are looking for\n",
    "# print(address.text)\n",
    "print(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all tags in the HTML\n",
    "tags = soup.find_all(True)\n",
    "\n",
    "# Iterate over each tag\n",
    "for tag in tags:\n",
    "    print(f\"Tag name: {tag.name}\")\n",
    "    print(f\"Attributes: {tag.attrs}\")\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame(columns=['Address', 'Price', 'Detail', 'Links'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_tag = soup.find('address', {'data-test': 'property-card-addr'})\n",
    "print(address_tag.cdata_list_attributes)\n",
    "\n",
    "# Print the address\n",
    "# print(address_tag.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all tags in the HTML\n",
    "tags = soup.find_all(True)\n",
    "\n",
    "# Iterate over each tag\n",
    "for tag in tags:\n",
    "    print(f\"Tag name: {tag.name}\")\n",
    "    # Iterate over each attribute\n",
    "    for attr, value in tag.attrs.items():\n",
    "        print(f\"Attribute: {attr}\")\n",
    "        print(f\"Value: {value}\")\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_tag = soup.find('address', {'data-test': 'property-card-addr'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses = soup.find_all('div', class_ = 'list-card-info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for house in houses:\n",
    "    print(f\"Tag name: {house.name}\")\n",
    "    print(f\"Attributes: {house.attrs}\")\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(houses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the body tag\n",
    "body = soup.body\n",
    "\n",
    "# Find all tags under the body\n",
    "tags = body.find_all(True)\n",
    "\n",
    "# Iterate over each tag\n",
    "for tag in tags:\n",
    "    print(f\"Tag name: {tag.name}\")\n",
    "    print(f\"Attributes: {tag.attrs}\")\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "address_tag = soup.find('address', {'data-test': 'property-card-addr'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(address_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(address_tag.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for house in houses:\n",
    "    address_tag = house.find('address', {'data-test': 'property-card-addr'})\n",
    "    print(address_tag)\n",
    "\n",
    "# # Print the address\n",
    "# print(address_tag.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for house in houses:\n",
    "    try:\n",
    "        address = house.find('address', {'class':'list-card-addr'}).text\n",
    "        price = house.find('div', {'class':'list-card-price'}).text\n",
    "        detail = house.find('ul', {'class':'list-card-details'}).text\n",
    "        link = house.a['href']\n",
    "        print(address)\n",
    "    except Exception as e:\n",
    "        address = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = table.append({'Address':address, 'Price':price, 'Detail':detail 'Links':link}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.to_csv('Redding.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zillow scraper using a paid API\n",
    "\"\"\"\n",
    "This is an example web scraper for zillow.com used in scrapfly blog article:\n",
    "https://scrapfly.io/blog/how-to-scrape-zillow/\n",
    "\n",
    "To run this scraper set env variable $SCRAPFLY_KEY with your scrapfly API key:\n",
    "$ export $SCRAPFLY_KEY=\"your key from https://scrapfly.io/dashboard\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install loguru\n",
    "# !pip install scrapy\n",
    "# !pip install scrapfly-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import json\n",
    "# import os\n",
    "# import random\n",
    "# import re\n",
    "# from typing import List\n",
    "# from urllib.parse import quote, urlencode\n",
    "\n",
    "# from loguru import logger as log\n",
    "# from scrapfly import ScrapeConfig, ScrapflyClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyPipeline(object):\n",
    "#     def process_item(self, item, spider):\n",
    "#         # Process the extracted data\n",
    "#         return item\n",
    "\n",
    "# class MySpider(scrapy.Spider):\n",
    "#     name = 'my_spider'\n",
    "#     start_urls = ['https://example.com']\n",
    "#     allowed_domains = ['example.com']\n",
    "#     custom_settings = {\n",
    "#         'ITEM_PIPELINES': {\n",
    "#             'your_project.pipelines.MyPipeline': 300,\n",
    "#         }\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# SCRAPFLY = ScrapflyClient(key=os.environ[\"SCRAPFLY_KEY\"])\n",
    "# BASE_CONFIG = {\n",
    "#     # Zillow.com requires Anti Scraping Protection bypass feature:\n",
    "#     \"asp\": True,\n",
    "#     \"country\": \"US\",\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def scrape_search(url: str) -> List[dict]:\n",
    "#     \"\"\"base search function which is used by sale and rent search functions\"\"\"\n",
    "#     log.info(f\"scraping search: {url}\")\n",
    "#     # first scrape the search HTML page and find query variables for this search\n",
    "#     html_result = await SCRAPFLY.async_scrape(ScrapeConfig(url, **BASE_CONFIG))\n",
    "#     script_data = json.loads(html_result.selector.xpath(\"//script[@id='__NEXT_DATA__']/text()\").get())\n",
    "#     query_data = script_data[\"props\"][\"pageProps\"][\"searchPageState\"][\"queryState\"]\n",
    "#     full_query = {\n",
    "#         \"searchQueryState\": query_data,\n",
    "#         \"wants\": {\"cat1\": [\"listResults\", \"mapResults\"], \"cat2\": [\"total\"]},\n",
    "#         \"requestId\": random.randint(2, 10),\n",
    "#     }\n",
    "#     # then scrape Zillow's backend API for all query results:\n",
    "#     _backend_url = \"https://www.zillow.com/async-create-search-page-state\"\n",
    "#     api_result = await SCRAPFLY.async_scrape(\n",
    "#         ScrapeConfig(_backend_url, **BASE_CONFIG, headers={\"content-type\": \"application/json\"},\n",
    "#                       body=json.dumps(full_query), method=\"PUT\")\n",
    "#     )\n",
    "#     data = json.loads(api_result.content)\n",
    "#     _total = data[\"categoryTotals\"][\"cat1\"][\"totalResultCount\"]\n",
    "#     if _total > 500:\n",
    "#         log.warning(f'more than 500 results ({_total}) for query \"{url}\" ')\n",
    "#     return data[\"cat1\"][\"searchResults\"][\"mapResults\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# async def scrape_properties(urls: List[str]):\n",
    "#     \"\"\"scrape zillow property pages for property data\"\"\"\n",
    "#     to_scrape = [ScrapeConfig(url, **BASE_CONFIG) for url in urls]\n",
    "#     results = []\n",
    "#     async for result in SCRAPFLY.concurrent_scrape(to_scrape):\n",
    "#         data = result.selector.css(\"script#__NEXT_DATA__::text\").get()\n",
    "#         if data:\n",
    "#             # Option 1: some properties are located in NEXT DATA cache\n",
    "#             data = json.loads(data)\n",
    "#             property_data = json.loads(data[\"props\"][\"pageProps\"][\"componentProps\"][\"gdpClientCache\"])\n",
    "#             property_data = property_data[list(property_data)[0]]['property']\n",
    "#         else:\n",
    "#             # Option 2: other times it's in Apollo cache\n",
    "#             data = result.selector.css(\"script#hdpApolloPreloadedData::text\").get()\n",
    "#             data = json.loads(json.loads(data)[\"apiCache\"])\n",
    "#             property_data = next(v[\"property\"] for k, v in data.items() if \"ForSale\" in k)\n",
    "#         results.append(property_data)\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
